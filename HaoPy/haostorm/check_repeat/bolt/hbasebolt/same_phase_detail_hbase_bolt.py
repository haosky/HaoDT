# -*- coding: utf-8 -*-from pyleus.storm import SimpleBoltimport tracebackfrom haohbase.hbase.ttypes import TColumnValuefrom pyleus.storm import namedtuplefrom datetime import datetimefrom haounits.loggerDefTools import get_defTestLoggerfrom haounits.loggerDefTools import get_defFileLogger as getlogfrom haounits.cfgOptionTools import get_items_in_cfg, get_uri_relative_parent_packageimport haostorm.check_repeat as mainparentfrom haohbase.hbase_dao import hbase_daoimport sysreload(sys)sys.setdefaultencoding('utf-8')__author__ = 'hao'log = getlog(fileName="same_phase_detail_hbase_bolt.log",loggerMain='same_phase_detail_hbase_bolt')# import logging# log = get_defTestLogger(level=logging.DEBUG)pass_obj = namedtuple("hbase","key value")class same_phase_detail_hbase_bolt(SimpleBolt):    OUTPUT_FIELDS = pass_obj    PHASE_TO_DOC_LEN = 4    PHASE_RAW_LEN = 8    '''    ->es_searcch_phase2doc_bolt    比较句子差异，并记录其他业务相关的字段信息    content input should be unicode    '''    def initialize(self):        log.info('start--;')        self.__sim_config_file_path = get_uri_relative_parent_package(mainparent, 'sim_settings.properties')        table_config = 'hbase_table'        self.exists_table = get_items_in_cfg(table_config, 'exists_table',                                             self.__sim_config_file_path)        self.phase_table = get_items_in_cfg(table_config, 'phase_table',                                            self.__sim_config_file_path)        log.info(self.exists_table)        log.info(self.phase_table)    def process_tuple(self, tup):        try:            data = tup.values            log.info(data)            sen_id = data[0]            gets = data[1]            # 原文章标题            project = gets[0]            # 相似句子相关字段信息            sam_detail = gets[1]            id_split = sen_id.split(':')            doc_uuid = id_split[0]            # 插入kafka，队列为docid            '''{                "same_doc_title": sam_doc_prop['project'],                "find_sam_sentence": left_sam_phase,                "sam_in_phase": "",                "doc_sentence": right_doc_phase,                "submiter": "某某",                "upload_at": sam_doc_prop['date'],                "wordcount": sam_doc_prop['content_len'],                "sim_rate_value": sim_rate_value,                "sim_rate": str(sim_rate_value)+'%',                "doc_sentence_word_count": right_doc_phase_word_count,                "sam_sentence_word_count": left_sam_phase_word_count,                "doc_sentence_id": id_split[1],                "sam_sentence_id": same_id,                "sentences_count":sentences_count,                "doc_content_len": sam_doc_prop['doc_content_len'],                "sam_content_len": sam_doc_prop['sam_content_len']            }'''            same_doc_title = TColumnValue(family='a', qualifier='same_doc_title', value= str(sam_detail['same_doc_title']).encode('utf8'))            find_sam_sentence = TColumnValue(family='a', qualifier='find_sam_sentence', value= str(sam_detail['find_sam_sentence']).encode('utf8'))            sam_in_phase = TColumnValue(family='a', qualifier='sam_in_phase', value= str(sam_detail['sam_in_phase']).encode('utf8'))            doc_sentence  = TColumnValue(family='a', qualifier='doc_sentence', value= str(sam_detail['doc_sentence']).encode('utf8'))            submiter = TColumnValue(family='a', qualifier='submiter', value= str(sam_detail['submiter']).encode('utf8'))            upload_at = TColumnValue(family='a', qualifier='upload_at', value= str(sam_detail['upload_at']).encode('utf8'))            wordcount = TColumnValue(family='a', qualifier='wordcount', value= str(sam_detail['wordcount']).encode('utf8'))            sim_rate_value = TColumnValue(family='a', qualifier='sim_rate_value', value= str(sam_detail['sim_rate_value']).encode('utf8'))            sim_rate = TColumnValue(family='a', qualifier='sim_rate', value= str(sam_detail['sim_rate']).encode('utf8'))            find_sentence = TColumnValue(family='a', qualifier='find_sentence', value= str(sam_detail['find_sentence']).encode('utf8'))            sentence = TColumnValue(family='a', qualifier='sentence', value= str(sam_detail['sentence']).encode('utf8'))            doc_sentence_word_count = TColumnValue(family='a', qualifier='doc_sentence_word_count',                                                   value=str(sam_detail['doc_sentence_word_count']).encode('utf8'))            sam_sentence_word_count = TColumnValue(family='a', qualifier='sam_sentence_word_count',                                                   value=str(sam_detail['sam_sentence_word_count']).encode('utf8'))            doc_sentence_id = TColumnValue(family='a', qualifier='doc_sentence_id',                                                   value= str(sam_detail['doc_sentence_id']).encode('utf8'))            sam_sentence_id = TColumnValue(family='a', qualifier='sam_sentence_id',                                                   value= str(sam_detail['sam_sentence_id']).encode('utf8'))            doc_title = TColumnValue(family='a', qualifier='doc_title',                                                   value= str(project).encode('utf8'))            doc_sentence_uuid = TColumnValue(family='a', qualifier='doc_sentence_uuid',                                                   value= str(sen_id).encode('utf8'))            sentences_count = TColumnValue(family='a', qualifier='sentences_count',                                             value=str(sam_detail['sentences_count']).encode('utf8'))            ssentence_id = sam_detail['sam_sentence_id']            ssentence_id_split = ssentence_id.split(':')            # 原文uuid+':'+相似文章uuid+':'+原文句子id+':'+相似句子id            rowkey = doc_uuid+':'+ssentence_id_split[0]+':'+id_split[1]+':'+ssentence_id_split[1]            columnValues=[doc_title,                          same_doc_title ,                          find_sam_sentence,                          sam_in_phase,                          doc_sentence,                          submiter,                          upload_at,                          wordcount,                          sim_rate_value,                          sim_rate,                          doc_sentence_word_count,                          sam_sentence_word_count,                          doc_sentence_id,                          sam_sentence_id,                          doc_sentence_uuid,                          sentences_count,                          sentence,                          find_sentence                          ]            self.hd = hbase_dao()            self.hd.put(self.phase_table,rowkey,columnValues)            self.hd.put(self.exists_table, doc_uuid, [TColumnValue(family='a', qualifier='docid',                                                                    value= str(doc_uuid).encode('utf8')),                                                             TColumnValue(family='a', qualifier='create_at',                                                                          value=str(datetime.now().strftime('%Y%m%d')).encode('utf8'))                                                             ])            self.hd.close()            result = (sen_id, gets)            log.info(result)            self.emit(result)        except Exception as e:            log.error(traceback.format_exc().replace('\n',' '))if __name__ == "__main__":    same_phase_detail_hbase_bolt().run()# @Test# from haostorm.testSite.SimpleBolt_MN import SimpleBolt# if __name__ == '__main__':#     es_sd = same_phase_detail_hbase_bolt()#     es_sd.initialize()#     class tup :#         values=(u'15994285707331215007fdac42722f20a5463be19e5ac5010c1c:149', [u'\u5173\u4e8e\u5b66\u4e60\u5ba3\u4f20\u548c\u8d2f\u5f7b\u5b9e\u65bd\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u6559\u80b2\u6cd5\u300b\u7684\u901a\u77e5', {'same_doc_title': u'\u5173\u4e8e\u5b66\u4e60\u5ba3\u4f20\u548c\u8d2f\u5f7b\u5b9e\u65bd\u300a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u6559\u80b2\u6cd5\u300b\u7684\u901a\u77e5', 'upload_at': u'\\N', 'wordcount': 2487, 'doc_sentence': u'<span class="sim gray">\u5b66\u4e60\u3001\u5ba3\u4f20\u4e2d\u7684\u6709\u5173\u53cd\u6620\u548c\u95ee\u9898\uff0c\u8bf7\u53ca\u65f6\u62a5\u9001\u56fd\u5bb6\u6559\u59d4</span>', 'doc_sentence_word_count': 24, 'sam_in_phase': '', 'sim_rate_value': 100.0, 'sam_sentence_id': u'15994285707331215007fdac42722f20a5463be19e5ac5010c1b:149', 'doc_sentence_id': u'149', 'submiter': u'\u67d0\u67d0', 'find_sam_sentence': u'<span class="lsim gray">\u5b66\u4e60\u3001\u5ba3\u4f20\u4e2d\u7684\u6709\u5173\u53cd\u6620\u548c\u95ee\u9898\uff0c\u8bf7\u53ca\u65f6\u62a5\u9001\u56fd\u5bb6\u6559\u59d4</span>', 'sim_rate': u'100.0%', 'sam_sentence_word_count': 24}])##     es_sd.process_tuple(tup())